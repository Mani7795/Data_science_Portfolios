{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba902694",
   "metadata": {},
   "source": [
    "# Steps in Problem Solving\n",
    "### Understanding the problem\n",
    "* Before solving the scenario we should be able to understand what the requirement is and identify the inputs, trying to figure out the expected output based on a few input values.\n",
    "\n",
    "### Breakdown it into smaller parts\n",
    "* Breaking down the main idea into smaller parts helps in organizing our steps and breaks down complex problems into smaller and easier segments.\n",
    "\n",
    "### Planning the methods and packages used\n",
    "* After breaking down the problem into smaller parts we will be importing the required packages and installing the software. And determine the most feasible solution for the problem.\n",
    "\n",
    "### Debugging\n",
    "* If we are not getting desired output on certain test cases or if we have any errors, we can debug the program and resolve the issues.\n",
    "\n",
    "### Review and optimize\n",
    "* Review the code and approach again to ensure correctness, efficiency, and readability. Check for the parts where we can optimize the code to improve the performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399562e7",
   "metadata": {},
   "source": [
    "# Learnings from Notebook\n",
    "\n",
    "* Jupyter Notebook helps us execute code in small segments. We can write the code and execute it and see the results instantly. If we have any errors in the code or if the end result does not meet the requirements we can modify and work on different techniques.\n",
    "* It can be used to write an explanation of the code right after the code snippet where we can use a markdown cell. In addition to that, the comments can be added while writing the code for better understanding.\n",
    "* Using a few libraries such as pandas, Matplotlib and Numpy can help us in performing tasks on datasets like cleaning, and generating plots for the data, and can be used for performing analytics tasks.\n",
    "* Jupyter Notebook is mainly used where the machine learning concepts are used as Python is mainly used script in it. And using jupyter notebook helps us iterate the code and fine-tune the solution.\n",
    "* Version Control: Using version control from git hub, to check and keep track of the code changes and the author of the code so that if any error or functionality goes wrong they can track back the changes. Using git hub can help 2 different people to work on the same code. This helps in code backup and collaboration on different features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943f5b2f",
   "metadata": {},
   "source": [
    "# Progress from Start\n",
    "\n",
    "* Since the beginning, I have been working on large amounts of data, mainly datasets. Transforming the datasets into suitable formats. And cleaning the data and filling out the missing values or removing those records from the dataset can help us for better decision making\n",
    "* Visualization of Data: Plotting the data can help us understand and identify similarities and find insights.\n",
    "* Training Models: techniques such as KNN, Gaussian Bayes, Cross-validation, Linear, and logistic regression are used for predicting the dependent variables.\n",
    "* Feature Selection: A correlation matrix is used in order to find out the correlation of all the features from the dataset and the features with the highest correlation are taken in order to determine dependent variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b2f09",
   "metadata": {},
   "source": [
    "# Using this learnings in future\n",
    "\n",
    "* Data science is a vast subject. Applications for using data science have increased exponentially in the past and will increase as time goes by. Using data we can find out trends in how people are going to react to particular products or any similar kind of services.\n",
    "* Using data science, as we have a basic idea of how the data cleaning, processing, and training of models works, using this we can train more complex models and even clean a very large dataset that can have 100s of features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fce6a3",
   "metadata": {},
   "source": [
    "# Reason for choosing house pricing dataset\n",
    "* One of the main reasons to choose the house pricing dataset is because in Sydney there has been a surge in rental crisis over the past 6 months and rent prices have increased drastically. According to the report, the rentals for houses have surged 24% compared to last year.\n",
    "* As we see a surge in rental prices we can also see the surge in house prices as well. So for this reason, wanted to design a model using machine learning to train the dataset and test it on the new data.\n",
    "* One more reason to select this dataset is, because of its high relevance to everybody. Almost everyone at least once in their lifetime will think of buying a house. Analyzing prices can help give insights to property holders, buyers, and investors.\n",
    "* It is a significant number of features in the dataset like square feet number of rooms and many others. Which can have a direct correlation with the price of the property. Working with such a feature-rich dataset can give more accurate prices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc02ec3f",
   "metadata": {},
   "source": [
    "# Problems solved during the portfolio\n",
    "* Using regression analysis, the model can be used to predict house prices from the given set of features and input variables.\n",
    "* By using this model, valuable insights for investors, house owners, buyers, and agents. The prices can help in pricing and analysis for investment.\n",
    "* Using this model can help us identify the overpricing of the property or underpriced assets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49bf758",
   "metadata": {},
   "source": [
    "# Insights and conclusion drawn from Portfolio 4\n",
    "* Data collection and processing\n",
    "  * Here the dataset is made of around 20 features or columns, which is about the number of rooms, area of the property, area of living room, number of bedrooms, and bathrooms.\n",
    "  * After data collection we are checking for missing values in the dataset, check for inconsistency, remove outliers, and visualize the data to see where most of the data is distributed across the range.\n",
    "* Model selection\n",
    "  * linear regression is used as it is simple and interpretable. This is used for getting linear relations between the input variable and the output variable. Using this model gave an accuracy of around 57%.\n",
    "  * K-nearest Neighbors to find out nonlinear relation between the features, Using this model gave an accuracy of 62%\n",
    "\n",
    "\n",
    "#### Conclusion\n",
    "* I had expected the model to have a better accuracy but maybe I have not done tuning of the hyper-parameter correctly. I think this model needs a little rework on that and reiterate the model by exploring more features that are impacting the output variables. \n",
    "\n",
    ". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ea30ba",
   "metadata": {},
   "source": [
    "\n",
    ". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d596804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
